# NetworkSecurity - Enterprise Phishing Detection System

[![CI/CD Pipeline](https://github.com/nasim-raj-laskar/Net-Sec/actions/workflows/main.yaml/badge.svg)](https://github.com/nasim-raj-laskar/Net-Sec/actions)
[![Code Quality](https://github.com/nasim-raj-laskar/Net-Sec/actions/workflows/code-quality.yaml/badge.svg)](https://github.com/nasim-raj-laskar/Net-Sec/actions)
[![Security Scan](https://github.com/nasim-raj-laskar/Net-Sec/actions/workflows/security-scan.yaml/badge.svg)](https://github.com/nasim-raj-laskar/Net-Sec/actions)
[![Testing](https://github.com/nasim-raj-laskar/Net-Sec/actions/workflows/testing.yaml/badge.svg)](https://github.com/nasim-raj-laskar/Net-Sec/actions)
[![Performance Test](https://github.com/nasim-raj-laskar/Net-Sec/actions/workflows/performance-test.yaml/badge.svg)](https://github.com/nasim-raj-laskar/Net-Sec/actions)
[![Docker](https://img.shields.io/badge/Docker-Available-blue?logo=docker)](https://hub.docker.com/r/nasimrajlaskar/networksecurity)
[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)](https://www.python.org/downloads/)
[![MLflow](https://img.shields.io/badge/MLflow-Tracking-orange)](https://dagshub.com/nasim-raj-laskar/Net-Sec)
[![AWS](https://img.shields.io/badge/AWS-S3%20%7C%20ECR%20%7C%20EC2-orange)](https://aws.amazon.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-Atlas-green)](https://www.mongodb.com/atlas)
[![FastAPI](https://img.shields.io/badge/FastAPI-Framework-teal)](https://fastapi.tiangolo.com/)

Production-grade MLOps system for phishing website detection leveraging modular architecture, automated CI/CD pipelines, cloud-native deployment, and comprehensive monitoring infrastructure.

## ðŸ—ï¸ System Architecture

### High-Level Architecture

![dataflow](static/sim.png)

### Modular Component Design

```
networksecurity/
â”œâ”€â”€ components/                # Core ML pipeline components
â”‚   â”œâ”€â”€ data_ingestion.py      # MongoDB â†’ Feature Store ETL
â”‚   â”œâ”€â”€ data_validation.py     # Schema validation & drift detection
â”‚   â”œâ”€â”€ data_transformation.py # Feature engineering & preprocessing
â”‚   â””â”€â”€ model_trainer.py       # Multi-algorithm training with MLflow
â”œâ”€â”€ pipeline/                  # Orchestration layer
â”‚   â”œâ”€â”€ training_pipeline.py   # End-to-end ML pipeline
â”‚   â””â”€â”€ batch_prediction.py    # Inference pipeline
â”œâ”€â”€ cloud/                     # Cloud integration
â”‚   â””â”€â”€ s3_syncer.py           # AWS S3 artifact synchronization
â”œâ”€â”€ utils/                     # Utility modules
â”‚   â”œâ”€â”€ ml_utils/              # ML-specific utilities
â”‚   â””â”€â”€ main_utils/            # General utilities
â””â”€â”€ entity/                    # Configuration & artifact entities
    â”œâ”€â”€ config_entity.py       # Pipeline configurations
    â””â”€â”€ artifact_entity.py     # Artifact definitions
```


### Data Flow Architecture

![dataflow](static/dataflow.png)

## ðŸš€ CI/CD Pipeline Architecture

### Multi-Stage Pipeline Configuration

#### 1. Continuous Integration (`main.yaml`)
- **Trigger**: Push to main branch (excluding README changes)
- **Stages**: Code linting, unit testing, security scanning
- **Permissions**: OIDC token-based AWS authentication

#### 2. Continuous Delivery (Disabled - Production Ready)
```yaml
build-and-push-ecr-image:
  - AWS ECR authentication via IAM roles
  - Docker containerization with multi-stage builds
  - Image tagging with semantic versioning
  - Automated push to ECR repository
```

#### 3. Continuous Deployment (Self-Hosted Runner)
```yaml
continuous-deployment:
  - Blue-green deployment strategy
  - Container orchestration with health checks
  - Automated rollback mechanisms
  - Environment variable injection
```

### Quality Assurance Workflows

#### Code Quality Pipeline (`code-quality.yaml`)
- **Black**: Code formatting enforcement
- **isort**: Import statement organization
- **flake8**: PEP8 compliance & complexity analysis
- **mypy**: Static type checking

#### Security Scanning (`security-scan.yaml`)
- **Bandit**: AST-based security vulnerability detection
- **Safety**: Known security vulnerability database checks
- **pip-audit**: Dependency vulnerability scanning
- **Scheduled scans**: Weekly automated security audits

#### Testing Framework (`testing.yaml`)
- **Multi-version testing**: Python 3.9, 3.10, 3.11
- **Coverage reporting**: Codecov integration
- **Integration testing**: API endpoint validation
- **Docker build verification**: Container integrity checks

## â˜ï¸ Cloud Infrastructure

### AWS Services Integration

#### Amazon ECR (Elastic Container Registry)
- **Purpose**: Docker image storage and versioning
- **Configuration**: Private repository with lifecycle policies
- **Security**: IAM-based access control with least privilege

#### Amazon EC2 (Elastic Compute Cloud)
- **Purpose**: Production application hosting and compute resources
- **Configuration**: Ubuntu 20.04 LTS with Python 3.9+ runtime
- **Security**: Security groups with restricted inbound access (SSH:22, HTTP:8080)
- **Deployment**: Docker containerization with automated CI/CD integration
- **Scaling**: Auto Scaling Groups for high availability and load distribution

#### Amazon S3 (Simple Storage Service)
- **Artifact Storage**: Model artifacts, preprocessors, training data
- **Bucket Structure**:
  ```
  s3://networksecurity010/
  â”œâ”€â”€ artifact/{timestamp}/     # Training artifacts
  â”œâ”€â”€ final_model/{timestamp}/  # Production models
  â””â”€â”€ data/                     # Raw datasets
  ```
- **Synchronization**: Automated via `S3Sync` utility class

#### MongoDB Atlas Integration
- **Database**: `NASIMRL`
- **Collection**: `NetworkData`
- **Connection**: SSL/TLS encrypted with certificate validation
- **Data Pipeline**: CSV â†’ JSON â†’ MongoDB document store

### Infrastructure as Code
```python
# S3 Synchronization Implementation
class S3Sync:
    def sync_folder_to_s3(self, folder, aws_bucket_name):
        command = f"aws s3 sync {folder} {aws_bucket_name}/"
        os.system(command)
```

## ðŸ”¬ MLOps & Experiment Tracking

### DagsHub Integration
- **Repository**: `nasim-raj-laskar/Net-Sec`
- **MLflow Backend**: Centralized experiment tracking
- **Model Registry**: Version-controlled model artifacts
- **Collaboration**: Team-based ML experiment management

### MLflow Experiment Tracking
```python
def track_mlflow(self, best_model, classificationmetric):
    with mlflow.start_run():
        mlflow.log_metric("f1_score", classificationmetric.f1_score)
        mlflow.log_metric("precision_score", classificationmetric.precision_score)
        mlflow.log_metric("recall_score", classificationmetric.recall_score)
        mlflow.sklearn.log_model(best_model, "model")
```

### Model Training Pipeline
- **Algorithms**: Random Forest, Decision Tree, Gradient Boosting, Logistic Regression, AdaBoost
- **Hyperparameter Tuning**: GridSearchCV with cross-validation
- **Model Selection**: Automated best model selection based on performance metrics
- **Artifact Management**: Automated model and preprocessor serialization

## ðŸ“Š Data Schema & Validation

### Feature Schema (`schema.yaml`)
```yaml
numerical_columns: [30 features]
  - having_IP_Address, URL_Length, Shortining_Service
  - SSL_final_State, Domain_registration_length
  - Page_Rank, Google_Index, Statistical_report
target_column: Result (Binary Classification)
```

### Data Validation Pipeline
- **Schema Validation**: Automated column type and constraint checking
- **Data Drift Detection**: Statistical drift analysis with YAML reporting
- **Quality Metrics**: Missing value analysis, outlier detection

## ðŸ³ Containerization & Deployment

### Docker Configuration
```dockerfile
FROM python:3.10-slim-buster
WORKDIR /app
COPY . /app
RUN pip install --no-cache-dir awscli
RUN pip install --no-cache-dir -r requirements.txt
CMD ["python3", "app.py"]
```

### Production Deployment
- **Container Orchestration**: Docker with health checks
- **Port Configuration**: 8080 (production), 8000 (development)
- **Environment Variables**: AWS credentials, MongoDB connection strings
- **Resource Management**: Memory and CPU limits via container constraints

## ðŸŒ Web Application Architecture

### FastAPI Backend
- **Framework**: FastAPI with async support
- **CORS**: Cross-origin resource sharing enabled
- **Static Files**: CSS, JS, and prediction outputs
- **Template Engine**: Jinja2 for dynamic HTML rendering

### API Endpoints
```python
@app.get("/")                    # Main application interface
@app.post("/predict")            # File upload and batch prediction
@app.get("/train")               # Model training trigger
```

### Frontend Features
- **Single Page Application**: Unified interface for all operations
- **File Upload**: CSV batch processing with validation
- **Real-time Training**: Progress tracking with WebSocket support
- **Results Visualization**: Enhanced table formatting with export capabilities

## ðŸ”§ Development Setup

### Prerequisites
- Python 3.9+
- MongoDB Atlas connection
- AWS CLI configured
- Docker 

### Installation
```bash
# Clone repository
git clone https://github.com/nasim-raj-laskar/Net-Sec.git
cd NetworkSecurity

# Install dependencies
pip install -r requirements.txt
pip install -e .

# Configure environment
cp .env.example .env
# Edit .env with your MongoDB and AWS credentials

# Initialize data
python push_data.py

# Start development server
python app.py
```

### Environment Variables
```bash
#.env updates(local)
MONGO_DB_URL=mongodb+srv://...
#update these in the githubs secrets
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
ECR_REPOSITORY_NAME=networksecurity

```

## ðŸ“ˆ Monitoring & Observability

### Logging Infrastructure
- **Structured Logging**: Timestamp-based log files in `/logs/`
- **Log Rotation**: Automated cleanup and archival
- **Error Tracking**: Exception handling with stack traces

### Performance Metrics
- **Model Performance**: F1-score, Precision, Recall tracking
- **System Metrics**: Response time, throughput monitoring
- **Resource Utilization**: Memory, CPU usage tracking

### Artifact Management
```
Artifact/{timestamp}/
â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ feature_store/phisingData.csv
â”‚   â””â”€â”€ ingested/{train,test}.csv
â”œâ”€â”€ data_validation/
â”‚   â”œâ”€â”€ drift_report/report.yaml
â”‚   â””â”€â”€ validated/{train,test}.csv
â”œâ”€â”€ data_transformation/
â”‚   â”œâ”€â”€ transformed/{train,test}.npy
â”‚   â””â”€â”€ transformed_object/preprocessing.pkl
â””â”€â”€ model_trainer/
    â””â”€â”€ trained_model/model.pkl
```

## ðŸš¦ Production Deployment

### Local Development
```bash
# Windows
start_server.bat

# Linux/Mac
chmod +x start_server.sh
./start_server.sh

# Manual
python app.py
```

### AWS EC2 Production Deployment

#### EC2 Instance Setup
```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install Python and dependencies
sudo apt install python3 python3-pip git -y

# Clone repository
git clone https://github.com/nasim-raj-laskar/Net-Sec.git
cd NetworkSecurity

# Install requirements
pip3 install -r requirements.txt

# Configure environment variables
export MONGO_DB_URL="your_mongodb_connection_string"
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"

# Start application
python3 app.py
```

#### Docker Deployment on EC2
```bash
# Install Docker
sudo apt install docker.io -y
sudo systemctl start docker
sudo systemctl enable docker

# Build and run container
sudo docker build -t networksecurity .
sudo docker run -d -p 8080:8080 \
  -e MONGO_DB_URL="your_mongodb_url" \
  -e AWS_ACCESS_KEY_ID="your_access_key" \
  -e AWS_SECRET_ACCESS_KEY="your_secret_key" \
  networksecurity
```

#### Security Group Configuration
- **Inbound Rules**:
  - HTTP: Port 80 (if using load balancer)
  - Custom TCP: Port 8080 (application port)
  - SSH: Port 22 (for management)

#### Access Application
- **Local**: http://localhost:8080
- **EC2**: http://your-ec2-public-ip:8080


## ðŸ“š Technical Documentation

### Model Documentation
- **Algorithm Selection**: Ensemble methods with cross-validation
- **Feature Engineering**: KNN imputation, standardization
- **Performance Benchmarks**: >85% accuracy on test dataset

### Security Considerations
- **Input Validation**: CSV schema enforcement
- **Authentication**: JWT-based API security (future enhancement)
- **Data Privacy**: PII anonymization in logs
- **Vulnerability Management**: Automated dependency scanning


### Code Standards
- Follow PEP8 guidelines
- Maintain >80% test coverage
- Update documentation for new features
- Pass all CI/CD pipeline checks

---